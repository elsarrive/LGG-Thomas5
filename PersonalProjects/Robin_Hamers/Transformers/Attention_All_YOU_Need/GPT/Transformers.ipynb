{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - Attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/de/42/ae30952c4a0bd773e90c9bf2579f5533037c886dfc8ec68133d5694f4dd2/sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-26 16:26:03--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-03-26 16:26:03 (9.13 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding= 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of dataset characters : 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Len of dataset characters : {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a vocabulary list of the characters used : (will not use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Encoder/Decoder (will not use this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"Hello World!\"))\n",
    "print(decode(encode(\"Hello World!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=input.txt --model_prefix=spm --vocab_size=5000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: input.txt\n",
      "  input_format: \n",
      "  model_prefix: spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: input.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9692% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999692\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563788\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33662 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25670\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25670 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12635 obj=11.7318 num_tokens=52726 num_tokens/piece=4.17301\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10764 obj=9.49501 num_tokens=53076 num_tokens/piece=4.93088\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8070 obj=9.50511 num_tokens=56370 num_tokens/piece=6.98513\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8067 obj=9.46589 num_tokens=56420 num_tokens/piece=6.99393\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6049 obj=9.65571 num_tokens=61693 num_tokens/piece=10.1989\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6048 obj=9.61202 num_tokens=61692 num_tokens/piece=10.2004\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5500 obj=9.67705 num_tokens=63428 num_tokens/piece=11.5324\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5500 obj=9.66335 num_tokens=63427 num_tokens/piece=11.5322\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: spm.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm.vocab\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "spm.SentencePieceTrainer.Train('--input=input.txt --model_prefix=spm --vocab_size=vocab_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁First', '▁', 'C', 'it', 'i', 'zen', ':', '▁Be', 'f', 'or', 'e', '▁we', '▁pr', 'o', 'ce', 'e', 'd', '▁an', 'y', '▁f', 'u', 'r', 'ther', ',', '▁', 'he', 'a', 'r', '▁me', '▁sp', 'eak', '.', '▁All', ':', '▁Speak', ',', '▁sp', 'eak', '.', '▁First', '▁Citizen', ':', '▁You']\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor(model_file='spm.model')\n",
    "for n in range(5):\n",
    "    a = s.encode(text[:100], out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the spm SentencePieceProcessor to encode/decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([535228]) torch.int64\n",
      "tensor([ 171,  370,    4, 1121,   14,  319,   87,  615,  624,  699,   45,  224,\n",
      "        1068,    3,   14,  363, 1416,   14, 1203,  151,    5,   65,  134,  134,\n",
      "           4, 1113,    3,  151,    5,  131,  547,   70,  370,    4,  118,   15,\n",
      "         181,   51,   14,  170,   87,    6, 1017,  637,   17,   14,  170, 2099,\n",
      "          10, 1193,   87,  123,   10,  676,  717, 1034,   19,   14,  159,   61,\n",
      "           4,  163,   87,    6, 1017,  402,   87,   17,    5,  992,  231,  272,\n",
      "         134,  637,   17,    5,  131,  547,    6,   47,   14,  488,  523, 1976,\n",
      "          75,    4,   14,  145,  547,   70,    3,   16,  108, 1809,  527,   14,\n",
      "         892,   14,  396, 1123,  353,  867, 1030,  272,    8, 1788, 1164,  271,\n",
      "           5,   14,  159,  134,  134,    4,  620,   87,  108,    9,   47,    3,\n",
      "         923,   87,   14,  417,   75,  272,  319,    9,   47,    5,  171,   14,\n",
      "         488,  401,   47,  401, 1781,    4,  269,  102,  502,   37,    3,   14,\n",
      "         390,   17,   55,    9,   61,   14,  363,  865,   14, 1261,  170,   75,\n",
      "          82,   14,  850,   14,  272,  319,   75, 1710,  401,  699,    5,    7,\n",
      "           6,    9,   47,   15,   14,  637,  170,   17,  747,   47,   19,  442,\n",
      "           4,   14,  303,  272,   14,  288,  272,  181,  544,   35,   14,  342,\n",
      "           9,   47,   12,   14,  271,   47,   14,  523,   27,   56,  934,    4,\n",
      "         318,    3,  318,   21,   14,  142,  287,   96,  523,  401, 2005,  247,\n",
      "           4,  815,   14,  319,  354,   17,    3,   72,   14,  304,  523,  401,\n",
      "        3382,   75,    6,    5,  171,   96,  523,  401, 3382,   75,    4,  194,\n",
      "          15,  181, 1606,   45,  309,  812,  523,  401, 3382,   75,    6,    3,\n",
      "          14,   47,  911, 2762,   14, 2004,  272,   17,    5,   73, 4556,  602,\n",
      "        3206,    6,  207,   75,  923, 1296, 4259,  102,    4,   84,    8,   80,\n",
      "         923, 1296,  761,   14, 4999,    6,   44,   14,   47,  363,   87, 4314,\n",
      "         602,    3,  594,  480,   47,   14, 1286,  181,   14,  319, 1525,  271,\n",
      "           6,  272,  288,   87,    3,   55,   14,  288, 1150, 1543,   99,  992,\n",
      "         560,  401,   87,  637,   17,  102, 2094,  543,   87,  134,   80,   12,\n",
      "          44,   14,   47,  363, 1464,  182,   55,   14,  375,   87,   14, 1173,\n",
      "         272,  654,  201,  170,    4,    8, 2052,  934,    6,    6,   14,   47,\n",
      "         363,  558, 2628,    6,   14,  505,    3,    8, 2932,   13,   62, 2313,\n",
      "           3,   25,   41,   15,   75,   20,  402,  729, 2395,   10, 2298,  892,\n",
      "          87,   94, 2249, 4999,   75,   17,  201,   75,  304,   87,   12,   14,\n",
      "         272, 4999,  170, 4023,   14,  401,    6,   14,  201, 1731,  331,   14,\n",
      "        1173,   90,  269,  102,  992, 2258,   75,  910,   14,   47,  363,  892,\n",
      "          29,  207,  667, 4861,    3,   14,   54,   87,   55,  996,   14,  170,\n",
      "         201,  417,   87,    6,    4,   31, 1030,  911,   14, 2004,   17,    6,\n",
      "          14,  417,   75,  736,    7,  151,   36,  480,   75, 2406,   54,   31,\n",
      "        3282,    3,   64,   47,   20,   14,  150,  401, 1996,   47,   31,  774,\n",
      "           5,   77,   87,  304,  342,   17,   14,  488,  401,   47,  401, 2005,\n",
      "         247,    4,  620, 1296,   16, 1367,  699,   87,   17,   14,  231, 1010,\n",
      "         304, 2508,   69,   15,  266,  201,  331,   70, 1809,  527,   19,   14,\n",
      "         159,   61,    4,   65,  742,   37,  398,    4,   33,    9,    6,   15,\n",
      "          14,  637,  836,   56,  266,   10,    8,  972,  288, 1174,   75,  276,\n",
      "         787,    5,   77,  287,  370,    4,   14,  488, 3752,   16,   14, 2401,\n",
      "          14,    6,   54,  402,  747,  231,   33,  584,   14,   17,  272,   75,\n",
      "          87,   31,   14,  363,  892,  657,   19,  131,  547,   70,   96,  401,\n",
      "          47, 1976,   75,    4, 2091,   80,   55,   61,   12,  135,   17,  812,\n",
      "         272, 4999,  679,   27,  744,   10,  167,   37,  685, 1318,   17,  542,\n",
      "         162,  354,   47,   31,   47,    3,   44,   22,   14,  911, 1122,    6,\n",
      "          37,  434,  134,  353,  923,  523,  363,   14, 1230,   35,  646,    5,\n",
      "          14,  142,  287,  370,    4,  350,    3,   14,  259, 4999,   47,  151,\n",
      "          14,   75, 1289, 2371, 2129,  134,   80,    5,  171,  370,    4,    7,\n",
      "          14,    6,  201,   80,  395,   16,    3,   67,   14,  363,   87,  113,\n",
      "         245, 2582,   69,    3,   14,  911,  146,   28,   10,   22,  465,    4,\n",
      "         407, 1074,   39,  304,  272,   75, 4131,   17,  210,  175,   14, 1230,\n",
      "         744,   10,   14,    6, 1031,   28,  923,  201,    6,   14,  931,   32,\n",
      "         657,   14,  363,   87,  146,  480,   47,   10,  541,   32,  891, 1289,\n",
      "         911,  170,   14,  390,   17, 1030,  272,   14, 1230,   14,  162,  201,\n",
      "         170,   47,  134,   80, 1367, 4999,   17,   12,  141,   33,   25,    3,\n",
      "         533,  405,    8,   15,  134, 1461,   47, 4999, 1149,   13,   32,  878,\n",
      "           5,   77,  287,  370,    4,   73,   14,  363,   87,  229,  471,   20,\n",
      "          32,   14,   75,  201, 1646,    3,   16, 1606,   15,   14,  402, 1073,\n",
      "         480,   75,   37,    5,  118,  891, 4999,    6,   47,   20,   14,   75,\n",
      "         272,   14,  319, 1031,   14,    6,  201,   80,   14,  363,   87,   25,\n",
      "         812,  272,  637,   47,  272,  505,    5,  131,  401, 1996,   47,   96,\n",
      "         401, 1461, 1781,    4,  143,   14,  237,  119,   24,    3,    7,  461,\n",
      "          14, 2857,   47,   27,  935,  170,  170,  247,   14, 1881, 2725,    6,\n",
      "          12,   33,   14,  363,  558,  363, 2228, 4999,  134,   47,    6,    3,\n",
      "          14, 2861,  363, 2700,  162,  134,  505,    3,   10,   14, 1461,  170,\n",
      "          87,   20,   14,  181,  162,   87, 1461,   47,  401,  272,   75,    5,\n",
      "         620,  363,  201,   47, 3296,    6,   63,  177,   19,   14, 3733,  290,\n",
      "          14, 1651,  207,    9,   14,  150,   87,  683,   25,   14,  170,  401,\n",
      "           6,   87,   75,    4,  453,  324,   55,   14,  162,  888, 1461, 1890,\n",
      "          97,   19,   10,   14,  150,   87, 1897,   21,  442,    4, 1870,  288,\n",
      "          87,    3,  972, 1203,    5,   14,  145,  401, 1996,   47,  370,    4,\n",
      "          77, 1881,   47,   21,  252,  972,  288,  231,   97,   19,   77,   87,\n",
      "        1261,   75,   17,   96,  401,   47, 1976,   75,    4, 2169,   80, 2702,\n",
      "         247,  401, 4999,    6,   65,  266,  477,  162,  162,  201,   12,   68,\n",
      "          87,   14,  150,  558,  113, 1815, 1281,   14,   47,  363,   87,  615,\n",
      "          87, 1164,  271,    5,   14,  145,  547,   70,  370,    4,  133,    9,\n",
      "           6,  207,  934,  912,  645,    4,   91,   51,   14,  150,   87,  542,\n",
      "          70,  120,   40,   21,   14,  376,  248,  568,  303,  117,    4,  620,\n",
      "         363,  558,   14, 3727,    9,    6,    3,  891,   80,  657, 1233,    3,\n",
      "          20,  220,   19,   14])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(s.encode(text, out_type=int, enable_sampling=True, alpha=0.1, nbest_size=-1), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of the block of text used to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 171,  370,    4, 1121,   14,  319,   87,  615,  624])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([171]), the target is 370\n",
      "When input is tensor([171, 370]), the target is 4\n",
      "When input is tensor([171, 370,   4]), the target is 1121\n",
      "When input is tensor([ 171,  370,    4, 1121]), the target is 14\n",
      "When input is tensor([ 171,  370,    4, 1121,   14]), the target is 319\n",
      "When input is tensor([ 171,  370,    4, 1121,   14,  319]), the target is 87\n",
      "When input is tensor([ 171,  370,    4, 1121,   14,  319,   87]), the target is 615\n",
      "When input is tensor([ 171,  370,    4, 1121,   14,  319,   87,  615]), the target is 624\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, the target is : {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[2264,  170, 3766,    5,   14,   74,  163,  368],\n",
      "        [ 402,   54,   80,  109,    3,  104,  275,  399],\n",
      "        [ 923, 1525,    3,  923,  401,   47,  363,   62],\n",
      "        [ 891, 1031,   27,   70,  477,   17,   87,    8]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 170, 3766,    5,   14,   74,  163,  368,  965],\n",
      "        [  54,   80,  109,    3,  104,  275,  399,  142],\n",
      "        [1525,    3,  923,  401,   47,  363,   62,  851],\n",
      "        [1031,   27,   70,  477,   17,   87,    8,   14]])\n",
      "--------\n",
      "When input is [2264], the target is : 170\n",
      "When input is [2264, 170], the target is : 3766\n",
      "When input is [2264, 170, 3766], the target is : 5\n",
      "When input is [2264, 170, 3766, 5], the target is : 14\n",
      "When input is [2264, 170, 3766, 5, 14], the target is : 74\n",
      "When input is [2264, 170, 3766, 5, 14, 74], the target is : 163\n",
      "When input is [2264, 170, 3766, 5, 14, 74, 163], the target is : 368\n",
      "When input is [2264, 170, 3766, 5, 14, 74, 163, 368], the target is : 965\n",
      "When input is [402], the target is : 54\n",
      "When input is [402, 54], the target is : 80\n",
      "When input is [402, 54, 80], the target is : 109\n",
      "When input is [402, 54, 80, 109], the target is : 3\n",
      "When input is [402, 54, 80, 109, 3], the target is : 104\n",
      "When input is [402, 54, 80, 109, 3, 104], the target is : 275\n",
      "When input is [402, 54, 80, 109, 3, 104, 275], the target is : 399\n",
      "When input is [402, 54, 80, 109, 3, 104, 275, 399], the target is : 142\n",
      "When input is [923], the target is : 1525\n",
      "When input is [923, 1525], the target is : 3\n",
      "When input is [923, 1525, 3], the target is : 923\n",
      "When input is [923, 1525, 3, 923], the target is : 401\n",
      "When input is [923, 1525, 3, 923, 401], the target is : 47\n",
      "When input is [923, 1525, 3, 923, 401, 47], the target is : 363\n",
      "When input is [923, 1525, 3, 923, 401, 47, 363], the target is : 62\n",
      "When input is [923, 1525, 3, 923, 401, 47, 363, 62], the target is : 851\n",
      "When input is [891], the target is : 1031\n",
      "When input is [891, 1031], the target is : 27\n",
      "When input is [891, 1031, 27], the target is : 70\n",
      "When input is [891, 1031, 27, 70], the target is : 477\n",
      "When input is [891, 1031, 27, 70, 477], the target is : 17\n",
      "When input is [891, 1031, 27, 70, 477, 17], the target is : 87\n",
      "When input is [891, 1031, 27, 70, 477, 17, 87], the target is : 8\n",
      "When input is [891, 1031, 27, 70, 477, 17, 87, 8], the target is : 14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a smal batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('--------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()}, the target is : {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create a simple bigram language model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5000])\n",
      "tensor(9.4888, grad_fn=<NllLossBackward0>)\n",
      " ⁇  Cheer chafe GrumioVOL fond weep trodage inform empty affect disguise aught Volscian Me executioner Pale volume bestow suddenlyread wherein date therefore blot Bianca shalt character beginningi pilgrimage writvoid canst spoken rein custom dreams Minola poHA expect Lodowick calf thy prescri fe Fortune joy destin jade accused Rome Condemnplotted depend wear damned slaughterthat state shelter mistrust corn whisper strait Offic grave defence Music puritIN suffice loss dastard unseen lords submissi already lust school supposed entertain verity skull Master rich reckon infringe Greekg did forsake home woful plainly hurt morrow calm choler\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T), tensors of integers\n",
    "        logits = self.token_embedding_table(idx) #(B,T,C) = batch (4) * time (8) * channel (vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) #Pytorch expect (B,C,T)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)  \n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape)  \n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(s.decode(m.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.22401237487793\n",
      "7.447227478027344\n",
      "7.111723899841309\n",
      "7.08571720123291\n",
      "7.330059051513672\n",
      "7.348686695098877\n",
      "7.220452308654785\n",
      "7.219447612762451\n",
      "7.108402729034424\n",
      "7.224458694458008\n",
      "7.388709545135498\n",
      "7.3378777503967285\n",
      "7.273087501525879\n",
      "7.1875128746032715\n",
      "7.053735733032227\n",
      "7.448727130889893\n",
      "7.173687934875488\n",
      "7.078999996185303\n",
      "7.227609157562256\n",
      "7.218874931335449\n",
      "7.308000087738037\n",
      "7.1261210441589355\n",
      "7.218200206756592\n",
      "7.220412731170654\n",
      "7.256505489349365\n",
      "7.099203586578369\n",
      "7.247226715087891\n",
      "7.046177387237549\n",
      "7.226519584655762\n",
      "7.234792232513428\n",
      "7.182720184326172\n",
      "7.253887176513672\n",
      "7.135412216186523\n",
      "7.270850658416748\n",
      "7.27522611618042\n",
      "7.2578630447387695\n",
      "7.286032676696777\n",
      "7.0365376472473145\n",
      "7.152425765991211\n",
      "7.200508117675781\n",
      "7.277250289916992\n",
      "7.279853343963623\n",
      "7.245128154754639\n",
      "7.217872142791748\n",
      "7.260928153991699\n",
      "7.193966865539551\n",
      "7.218711853027344\n",
      "7.349695682525635\n",
      "7.132151126861572\n",
      "7.2552385330200195\n",
      "7.1605048179626465\n",
      "7.26513147354126\n",
      "7.273092269897461\n",
      "7.220922470092773\n",
      "7.247310161590576\n",
      "7.1920247077941895\n",
      "7.322460651397705\n",
      "7.3318772315979\n",
      "7.255736351013184\n",
      "7.231245994567871\n",
      "7.122600555419922\n",
      "7.220174312591553\n",
      "7.055039882659912\n",
      "7.0832061767578125\n",
      "7.269216060638428\n",
      "7.047030925750732\n",
      "7.231780529022217\n",
      "7.0580153465271\n",
      "7.180123805999756\n",
      "7.048584461212158\n",
      "7.161790370941162\n",
      "7.0877790451049805\n",
      "7.142134666442871\n",
      "7.191969871520996\n",
      "7.166428089141846\n",
      "7.223901748657227\n",
      "7.224052429199219\n",
      "7.143728733062744\n",
      "7.1521453857421875\n",
      "7.223056316375732\n",
      "7.18131685256958\n",
      "7.144022464752197\n",
      "7.1938982009887695\n",
      "7.222743511199951\n",
      "7.108896255493164\n",
      "7.338245868682861\n",
      "7.33347225189209\n",
      "7.084680557250977\n",
      "7.168781280517578\n",
      "7.11250638961792\n",
      "7.07808256149292\n",
      "7.307204723358154\n",
      "7.142165660858154\n",
      "7.305130958557129\n",
      "7.0559797286987305\n",
      "7.081547260284424\n",
      "7.310179233551025\n",
      "7.2842936515808105\n",
      "7.274336338043213\n",
      "7.168795585632324\n",
      "7.134476184844971\n",
      "7.103893280029297\n",
      "7.252967834472656\n",
      "7.237293720245361\n",
      "7.224300384521484\n",
      "7.199820518493652\n",
      "7.011962890625\n",
      "7.2062907218933105\n",
      "7.090585708618164\n",
      "7.231779098510742\n",
      "7.1451029777526855\n",
      "7.22152042388916\n",
      "6.962248802185059\n",
      "7.0607523918151855\n",
      "7.2197160720825195\n",
      "7.101120948791504\n",
      "7.069756984710693\n",
      "7.152546405792236\n",
      "7.131172180175781\n",
      "7.070763111114502\n",
      "7.035065650939941\n",
      "7.0182623863220215\n",
      "7.046825408935547\n",
      "7.263914585113525\n",
      "7.243382453918457\n",
      "7.107824325561523\n",
      "7.342520713806152\n",
      "7.271503448486328\n",
      "7.1694440841674805\n",
      "7.126794338226318\n",
      "7.213817596435547\n",
      "7.193162441253662\n",
      "7.2349534034729\n",
      "7.113499641418457\n",
      "7.206236839294434\n",
      "7.0450286865234375\n",
      "7.041023254394531\n",
      "7.1116156578063965\n",
      "6.9957380294799805\n",
      "7.138017177581787\n",
      "7.100305080413818\n",
      "7.06462287902832\n",
      "6.99982213973999\n",
      "7.1054158210754395\n",
      "7.321168899536133\n",
      "7.0381340980529785\n",
      "7.504637718200684\n",
      "6.896495342254639\n",
      "7.0263566970825195\n",
      "7.139566898345947\n",
      "7.093005657196045\n",
      "7.20574426651001\n",
      "7.018795967102051\n",
      "7.010875225067139\n",
      "7.189809322357178\n",
      "7.331419944763184\n",
      "7.227838516235352\n",
      "7.110627174377441\n",
      "6.961324691772461\n",
      "7.155263423919678\n",
      "7.22754430770874\n",
      "7.095419883728027\n",
      "7.112612247467041\n",
      "7.2116265296936035\n",
      "7.274016380310059\n",
      "7.0689520835876465\n",
      "7.223745822906494\n",
      "7.132399082183838\n",
      "7.084367752075195\n",
      "7.122302055358887\n",
      "6.94082498550415\n",
      "7.178030014038086\n",
      "7.070018291473389\n",
      "6.956164836883545\n",
      "7.1248040199279785\n",
      "7.109728813171387\n",
      "7.076852321624756\n",
      "7.285100936889648\n",
      "7.219960689544678\n",
      "7.071080684661865\n",
      "7.105307579040527\n",
      "7.0828680992126465\n",
      "7.109346866607666\n",
      "6.977940559387207\n",
      "7.191141605377197\n",
      "6.9695024490356445\n",
      "7.037972450256348\n",
      "7.25675630569458\n",
      "7.27863883972168\n",
      "7.01387882232666\n",
      "6.966888427734375\n",
      "7.155772686004639\n",
      "7.083822250366211\n",
      "7.176647186279297\n",
      "7.2491374015808105\n",
      "6.98708438873291\n",
      "7.036706924438477\n",
      "7.0825300216674805\n",
      "6.877439498901367\n",
      "7.114208221435547\n",
      "7.092373371124268\n",
      "7.1366963386535645\n",
      "6.972169399261475\n",
      "6.971497535705566\n",
      "7.055508613586426\n",
      "7.081173419952393\n",
      "7.121010780334473\n",
      "7.18083381652832\n",
      "7.006834506988525\n",
      "7.119177341461182\n",
      "7.219198226928711\n",
      "6.947915554046631\n",
      "7.11427116394043\n",
      "7.04793643951416\n",
      "7.043403148651123\n",
      "7.08541202545166\n",
      "7.105601787567139\n",
      "7.076205730438232\n",
      "7.087838649749756\n",
      "7.010946750640869\n",
      "7.2168426513671875\n",
      "7.030176639556885\n",
      "7.075300693511963\n",
      "6.899700164794922\n",
      "7.066399574279785\n",
      "6.9463019371032715\n",
      "7.174566268920898\n",
      "6.896129608154297\n",
      "6.979204177856445\n",
      "7.211235523223877\n",
      "7.05279016494751\n",
      "7.10740327835083\n",
      "7.238066673278809\n",
      "7.097318172454834\n",
      "7.108860015869141\n",
      "6.90949821472168\n",
      "6.985558032989502\n",
      "7.136724472045898\n",
      "7.204507827758789\n",
      "7.006460666656494\n",
      "6.9664177894592285\n",
      "7.260848522186279\n",
      "7.244124412536621\n",
      "6.990784645080566\n",
      "7.05697774887085\n",
      "7.066035747528076\n",
      "7.071310043334961\n",
      "7.009357929229736\n",
      "7.016422748565674\n",
      "7.096272945404053\n",
      "6.983633995056152\n",
      "7.049496650695801\n",
      "7.11721134185791\n",
      "6.936535358428955\n",
      "7.123466491699219\n",
      "7.137414455413818\n",
      "7.28200626373291\n",
      "7.09389066696167\n",
      "7.1331963539123535\n",
      "7.086127281188965\n",
      "6.8882341384887695\n",
      "6.987345218658447\n",
      "6.948644161224365\n",
      "7.221264839172363\n",
      "7.030186176300049\n",
      "7.058983325958252\n",
      "7.277510643005371\n",
      "7.069878578186035\n",
      "7.351052284240723\n",
      "7.056244373321533\n",
      "7.115400791168213\n",
      "6.964526653289795\n",
      "7.204233169555664\n",
      "6.910397052764893\n",
      "7.068830966949463\n",
      "7.030740737915039\n",
      "7.016744136810303\n",
      "6.993688106536865\n",
      "7.145910739898682\n",
      "7.037705898284912\n",
      "6.899888038635254\n",
      "7.034725666046143\n",
      "7.040879726409912\n",
      "7.032867908477783\n",
      "7.180330753326416\n",
      "7.056263446807861\n",
      "6.811895847320557\n",
      "7.007936954498291\n",
      "7.09834098815918\n",
      "7.096156597137451\n",
      "7.166705131530762\n",
      "7.152203559875488\n",
      "7.104841232299805\n",
      "6.905111312866211\n",
      "7.061010360717773\n",
      "6.9224748611450195\n",
      "6.963226318359375\n",
      "6.992203712463379\n",
      "7.05653190612793\n",
      "6.983072280883789\n",
      "6.956040382385254\n",
      "7.162410736083984\n",
      "7.056852340698242\n",
      "7.057468414306641\n",
      "6.892313480377197\n",
      "6.967751979827881\n",
      "6.981563091278076\n",
      "6.995516777038574\n",
      "7.105936050415039\n",
      "6.989523887634277\n",
      "7.03148078918457\n",
      "6.811826705932617\n",
      "7.06463623046875\n",
      "7.026045322418213\n",
      "7.146892070770264\n",
      "6.854730606079102\n",
      "7.079980373382568\n",
      "7.068289279937744\n",
      "7.087892532348633\n",
      "7.038342475891113\n",
      "6.927951812744141\n",
      "6.941932201385498\n",
      "7.077583312988281\n",
      "6.9231157302856445\n",
      "6.993233680725098\n",
      "6.995977878570557\n",
      "7.004268169403076\n",
      "7.092662811279297\n",
      "7.061561584472656\n",
      "6.873161315917969\n",
      "6.942440509796143\n",
      "7.108042240142822\n",
      "6.994975566864014\n",
      "7.010879993438721\n",
      "7.066923141479492\n",
      "6.8769073486328125\n",
      "6.930239200592041\n",
      "7.027159214019775\n",
      "7.043843746185303\n",
      "7.0702805519104\n",
      "7.080099582672119\n",
      "6.983211994171143\n",
      "7.061844825744629\n",
      "6.878823757171631\n",
      "6.9044084548950195\n",
      "7.043961048126221\n",
      "7.0550665855407715\n",
      "6.9086174964904785\n",
      "7.011180400848389\n",
      "7.018332481384277\n",
      "7.143461227416992\n",
      "6.956090450286865\n",
      "6.969174385070801\n",
      "6.968176364898682\n",
      "6.705239295959473\n",
      "6.894954204559326\n",
      "7.14140510559082\n",
      "6.943089962005615\n",
      "7.03997802734375\n",
      "7.029854774475098\n",
      "6.9679856300354\n",
      "6.851817607879639\n",
      "7.022291660308838\n",
      "6.964845657348633\n",
      "7.149193286895752\n",
      "6.957381248474121\n",
      "7.054415702819824\n",
      "6.9996113777160645\n",
      "6.875753402709961\n",
      "6.969376564025879\n",
      "6.922266960144043\n",
      "7.009009838104248\n",
      "7.01125431060791\n",
      "7.0403852462768555\n",
      "7.051586627960205\n",
      "7.053949356079102\n",
      "6.985703468322754\n",
      "6.975882530212402\n",
      "6.998227596282959\n",
      "7.132390975952148\n",
      "7.019101619720459\n",
      "7.17295503616333\n",
      "7.093380928039551\n",
      "6.953161239624023\n",
      "7.088938236236572\n",
      "6.992367744445801\n",
      "6.979355335235596\n",
      "6.872150421142578\n",
      "7.0862579345703125\n",
      "7.0515241622924805\n",
      "7.143191337585449\n",
      "6.887580394744873\n",
      "6.995088577270508\n",
      "7.036378383636475\n",
      "7.036509990692139\n",
      "6.912736415863037\n",
      "7.009156703948975\n",
      "7.055755138397217\n",
      "6.927005290985107\n",
      "6.811149597167969\n",
      "7.036813259124756\n",
      "6.892199516296387\n",
      "6.778964519500732\n",
      "6.924305438995361\n",
      "6.890777111053467\n",
      "7.022243022918701\n",
      "6.992685794830322\n",
      "7.062196254730225\n",
      "7.112487316131592\n",
      "6.9386467933654785\n",
      "7.000492095947266\n",
      "7.0130510330200195\n",
      "7.118569850921631\n",
      "6.839792728424072\n",
      "6.88103723526001\n",
      "7.026513576507568\n",
      "7.081451892852783\n",
      "7.078306198120117\n",
      "6.851432800292969\n",
      "6.921333312988281\n",
      "6.9994916915893555\n",
      "6.8738884925842285\n",
      "7.1321258544921875\n",
      "7.043272972106934\n",
      "6.8862457275390625\n",
      "7.023590087890625\n",
      "6.994150161743164\n",
      "6.9031662940979\n",
      "7.107776641845703\n",
      "7.026744365692139\n",
      "7.0039896965026855\n",
      "6.985440731048584\n",
      "6.906862735748291\n",
      "7.00493860244751\n",
      "6.966226577758789\n",
      "6.985926628112793\n",
      "7.138660907745361\n",
      "6.966078281402588\n",
      "7.152363300323486\n",
      "6.948363780975342\n",
      "7.02427339553833\n",
      "7.073401927947998\n",
      "6.935880184173584\n",
      "7.128299236297607\n",
      "6.883367538452148\n",
      "7.042168140411377\n",
      "6.820901393890381\n",
      "7.136436939239502\n",
      "6.894880294799805\n",
      "6.900949954986572\n",
      "6.957904815673828\n",
      "6.936025142669678\n",
      "6.9709296226501465\n",
      "6.964850902557373\n",
      "7.07029390335083\n",
      "6.93646764755249\n",
      "6.989869594573975\n",
      "6.86238956451416\n",
      "6.973913192749023\n",
      "6.939980983734131\n",
      "7.085098743438721\n",
      "6.895994186401367\n",
      "6.903739929199219\n",
      "7.199291229248047\n",
      "6.901132106781006\n",
      "7.185577392578125\n",
      "6.971981048583984\n",
      "6.951282501220703\n",
      "7.031096458435059\n",
      "6.899892330169678\n",
      "7.037445068359375\n",
      "6.951557636260986\n",
      "6.821946144104004\n",
      "6.976850986480713\n",
      "7.046937942504883\n",
      "7.076618671417236\n",
      "6.883983612060547\n",
      "7.039906978607178\n",
      "7.039956569671631\n",
      "6.957953453063965\n",
      "7.00405740737915\n",
      "6.937530040740967\n",
      "6.8997111320495605\n",
      "7.032613277435303\n",
      "6.9082560539245605\n",
      "7.108630657196045\n",
      "6.963671684265137\n",
      "6.914933681488037\n",
      "7.023175239562988\n",
      "6.907148361206055\n",
      "6.985852241516113\n",
      "6.906491756439209\n",
      "7.107568740844727\n",
      "6.892938137054443\n",
      "6.801886081695557\n",
      "6.978801250457764\n",
      "7.03205680847168\n",
      "7.039922714233398\n",
      "7.069975852966309\n",
      "6.902166366577148\n",
      "6.79006814956665\n",
      "6.90603494644165\n",
      "6.830437183380127\n",
      "7.054117679595947\n",
      "7.057926654815674\n",
      "6.86984395980835\n",
      "6.806955337524414\n",
      "6.99336576461792\n",
      "6.866386413574219\n",
      "7.003429889678955\n",
      "6.71629524230957\n",
      "7.022162437438965\n",
      "6.874596118927002\n",
      "6.857850074768066\n",
      "6.9234299659729\n",
      "6.925730228424072\n",
      "7.040596008300781\n",
      "6.988151550292969\n",
      "7.041708469390869\n",
      "7.085899353027344\n",
      "7.1601738929748535\n",
      "7.027445316314697\n",
      "6.990265369415283\n",
      "6.805129051208496\n",
      "7.169466495513916\n",
      "6.954653739929199\n",
      "6.969377040863037\n",
      "7.006483554840088\n",
      "6.833845138549805\n",
      "7.0292534828186035\n",
      "6.846882343292236\n",
      "7.031080722808838\n",
      "6.877197742462158\n",
      "7.0966477394104\n",
      "6.9081549644470215\n",
      "6.786404609680176\n",
      "6.859615802764893\n",
      "7.186513900756836\n",
      "6.893220901489258\n",
      "6.955333232879639\n",
      "7.049234390258789\n",
      "6.874302387237549\n",
      "7.063631057739258\n",
      "7.089935779571533\n",
      "7.075324058532715\n",
      "7.1358642578125\n",
      "6.929586410522461\n",
      "6.818263530731201\n",
      "6.980493545532227\n",
      "6.957970142364502\n",
      "6.916087627410889\n",
      "6.720364570617676\n",
      "6.805922031402588\n",
      "6.820088863372803\n",
      "6.9698662757873535\n",
      "6.839354991912842\n",
      "6.932976245880127\n",
      "6.78493070602417\n",
      "6.6996893882751465\n",
      "6.801710605621338\n",
      "6.919745922088623\n",
      "6.7708330154418945\n",
      "6.869843006134033\n",
      "6.876537322998047\n",
      "6.951720714569092\n",
      "6.935890197753906\n",
      "6.833334445953369\n",
      "6.831287860870361\n",
      "7.134527683258057\n",
      "6.787431240081787\n",
      "6.756038188934326\n",
      "6.86043643951416\n",
      "6.8650593757629395\n",
      "6.912946701049805\n",
      "7.058635234832764\n",
      "6.970254421234131\n",
      "6.999222278594971\n",
      "6.840010643005371\n",
      "6.900645732879639\n",
      "6.893607139587402\n",
      "6.8376264572143555\n",
      "6.87764835357666\n",
      "6.898952484130859\n",
      "6.86939811706543\n",
      "6.7395172119140625\n",
      "6.778995037078857\n",
      "6.765268802642822\n",
      "7.036633491516113\n",
      "6.882683277130127\n",
      "7.056272029876709\n",
      "6.761141300201416\n",
      "6.875524997711182\n",
      "6.917355060577393\n",
      "6.769152641296387\n",
      "6.783023357391357\n",
      "6.80139684677124\n",
      "7.037765979766846\n",
      "7.044785499572754\n",
      "6.865004062652588\n",
      "6.777182579040527\n",
      "6.758066177368164\n",
      "6.7877044677734375\n",
      "6.911689281463623\n",
      "6.7456183433532715\n",
      "6.957061767578125\n",
      "6.8340840339660645\n",
      "6.803677082061768\n",
      "6.832094192504883\n",
      "6.73637056350708\n",
      "6.764122009277344\n",
      "6.912848472595215\n",
      "6.789458274841309\n",
      "6.7052788734436035\n",
      "7.079536437988281\n",
      "6.847712516784668\n",
      "6.82194709777832\n",
      "6.830113887786865\n",
      "6.840933322906494\n",
      "6.734156131744385\n",
      "6.940974712371826\n",
      "6.798583030700684\n",
      "6.736173152923584\n",
      "6.7910966873168945\n",
      "6.853679180145264\n",
      "6.75375509262085\n",
      "7.0746541023254395\n",
      "6.810986042022705\n",
      "6.759032249450684\n",
      "6.764407634735107\n",
      "7.043817043304443\n",
      "6.946585178375244\n",
      "6.811343193054199\n",
      "6.705976963043213\n",
      "6.905195236206055\n",
      "6.773406505584717\n",
      "6.950602054595947\n",
      "6.788607120513916\n",
      "7.0896100997924805\n",
      "6.874348163604736\n",
      "6.865729808807373\n",
      "6.805004119873047\n",
      "6.938404083251953\n",
      "6.846469402313232\n",
      "6.71200704574585\n",
      "6.708686351776123\n",
      "6.87497615814209\n",
      "6.890902042388916\n",
      "6.957828044891357\n",
      "6.715559005737305\n",
      "6.800628185272217\n",
      "6.743677139282227\n",
      "7.0214152336120605\n",
      "6.970433235168457\n",
      "6.973799705505371\n",
      "6.9180216789245605\n",
      "6.916269779205322\n",
      "6.62899112701416\n",
      "6.69494104385376\n",
      "6.92518424987793\n",
      "6.959357738494873\n",
      "6.9022417068481445\n",
      "7.033168315887451\n",
      "6.9730753898620605\n",
      "6.682437419891357\n",
      "6.661687850952148\n",
      "6.860812187194824\n",
      "6.8358893394470215\n",
      "6.6812896728515625\n",
      "6.8040995597839355\n",
      "6.999724864959717\n",
      "6.850717067718506\n",
      "6.896927356719971\n",
      "6.86701774597168\n",
      "6.7763261795043945\n",
      "6.857972621917725\n",
      "6.869692325592041\n",
      "6.843167304992676\n",
      "6.768804550170898\n",
      "6.7372541427612305\n",
      "6.840926647186279\n",
      "6.881611347198486\n",
      "7.0488104820251465\n",
      "6.88320779800415\n",
      "6.748763561248779\n",
      "6.8788251876831055\n",
      "6.939661026000977\n",
      "6.900721549987793\n",
      "6.8933281898498535\n",
      "6.78159236907959\n",
      "6.9508819580078125\n",
      "6.776972770690918\n",
      "6.886501789093018\n",
      "6.618373870849609\n",
      "6.771034240722656\n",
      "6.784350395202637\n",
      "6.977120876312256\n",
      "6.9589738845825195\n",
      "6.7901997566223145\n",
      "6.825995922088623\n",
      "6.860991477966309\n",
      "6.58376932144165\n",
      "6.960780143737793\n",
      "6.739795684814453\n",
      "6.820069313049316\n",
      "6.98004674911499\n",
      "6.706221580505371\n",
      "6.820010662078857\n",
      "6.825308322906494\n",
      "6.911904335021973\n",
      "6.795940399169922\n",
      "6.77475643157959\n",
      "6.615511894226074\n",
      "6.7547478675842285\n",
      "7.071535110473633\n",
      "6.98127555847168\n",
      "6.911483287811279\n",
      "6.7898664474487305\n",
      "6.716251373291016\n",
      "6.742988586425781\n",
      "6.816256523132324\n",
      "6.792235851287842\n",
      "6.750895977020264\n",
      "6.896514892578125\n",
      "6.973461627960205\n",
      "6.879336833953857\n",
      "6.892725944519043\n",
      "6.854156494140625\n",
      "6.8098015785217285\n",
      "6.890708923339844\n",
      "6.829069137573242\n",
      "6.756049156188965\n",
      "6.660425186157227\n",
      "6.935418128967285\n",
      "6.858863830566406\n",
      "6.80255651473999\n",
      "6.848562240600586\n",
      "6.886887073516846\n",
      "6.8102850914001465\n",
      "6.808372497558594\n",
      "6.805529594421387\n",
      "7.086602210998535\n",
      "6.612311840057373\n",
      "6.8219146728515625\n",
      "6.638179302215576\n",
      "6.768215656280518\n",
      "6.831730365753174\n",
      "6.767050743103027\n",
      "6.886119365692139\n",
      "6.646885395050049\n",
      "6.778335094451904\n",
      "6.66309928894043\n",
      "6.664775848388672\n",
      "6.8941497802734375\n",
      "6.787442684173584\n",
      "6.813785552978516\n",
      "6.9074859619140625\n",
      "6.798710346221924\n",
      "6.742933750152588\n",
      "6.764730930328369\n",
      "6.718730926513672\n",
      "6.763216018676758\n",
      "6.652798652648926\n",
      "6.779548645019531\n",
      "6.763767242431641\n",
      "6.785511016845703\n",
      "6.933380126953125\n",
      "6.660397052764893\n",
      "6.736886978149414\n",
      "6.795499801635742\n",
      "6.862093925476074\n",
      "6.8430705070495605\n",
      "6.777883529663086\n",
      "6.919787406921387\n",
      "6.818850040435791\n",
      "6.598231792449951\n",
      "6.823179721832275\n",
      "6.717007160186768\n",
      "6.684935569763184\n",
      "6.832274436950684\n",
      "6.823667049407959\n",
      "6.751039981842041\n",
      "6.882110118865967\n",
      "6.751041412353516\n",
      "6.914950847625732\n",
      "6.958279609680176\n",
      "6.839699745178223\n",
      "6.995112895965576\n",
      "6.657537460327148\n",
      "6.805699825286865\n",
      "6.558463096618652\n",
      "6.787871837615967\n",
      "6.73203182220459\n",
      "6.904417991638184\n",
      "6.973343372344971\n",
      "6.731083393096924\n",
      "6.772443771362305\n",
      "6.916352272033691\n",
      "6.781109809875488\n",
      "6.827660083770752\n",
      "6.6327385902404785\n",
      "6.831909656524658\n",
      "6.819693088531494\n",
      "6.768352508544922\n",
      "6.658111095428467\n",
      "6.885167598724365\n",
      "6.797661781311035\n",
      "6.725525379180908\n",
      "6.927718162536621\n",
      "6.799513816833496\n",
      "6.800273895263672\n",
      "6.783833026885986\n",
      "6.6845903396606445\n",
      "6.778345584869385\n",
      "6.792104244232178\n",
      "6.763291835784912\n",
      "6.669910430908203\n",
      "6.702597141265869\n",
      "6.759376049041748\n",
      "6.723567962646484\n",
      "6.756795883178711\n",
      "6.9157938957214355\n",
      "6.6667094230651855\n",
      "6.815803527832031\n",
      "6.553319931030273\n",
      "6.7795257568359375\n",
      "6.710309982299805\n",
      "6.843593597412109\n",
      "6.7317423820495605\n",
      "6.713337421417236\n",
      "6.721918106079102\n",
      "6.7266974449157715\n",
      "6.559877395629883\n",
      "6.863471984863281\n",
      "6.552064418792725\n",
      "6.615950584411621\n",
      "6.790865898132324\n",
      "6.7807936668396\n",
      "6.815244197845459\n",
      "6.821282386779785\n",
      "6.67105770111084\n",
      "6.762368202209473\n",
      "6.7392706871032715\n",
      "6.6871724128723145\n",
      "6.762779235839844\n",
      "6.729608058929443\n",
      "6.816502571105957\n",
      "6.851912021636963\n",
      "6.811079502105713\n",
      "6.7309184074401855\n",
      "6.71107816696167\n",
      "6.7924723625183105\n",
      "6.732571601867676\n",
      "6.66098690032959\n",
      "6.8376970291137695\n",
      "6.946134567260742\n",
      "6.693094730377197\n",
      "6.651914119720459\n",
      "6.7866973876953125\n",
      "6.641048431396484\n",
      "6.735677719116211\n",
      "6.696341514587402\n",
      "6.947136402130127\n",
      "6.6976752281188965\n",
      "6.8885955810546875\n",
      "6.717097759246826\n",
      "6.585508823394775\n",
      "6.84209680557251\n",
      "6.500759124755859\n",
      "6.773350238800049\n",
      "6.70922327041626\n",
      "6.698946475982666\n",
      "6.722230434417725\n",
      "6.616922855377197\n",
      "6.517865180969238\n",
      "6.647549152374268\n",
      "6.795082092285156\n",
      "6.6674089431762695\n",
      "6.644643306732178\n",
      "6.626807689666748\n",
      "6.856360912322998\n",
      "6.729763507843018\n",
      "6.713416576385498\n",
      "6.7267842292785645\n",
      "6.936233997344971\n",
      "6.711831569671631\n",
      "6.872943878173828\n",
      "6.816982269287109\n",
      "6.643557548522949\n",
      "6.580621719360352\n",
      "6.890600204467773\n",
      "6.569126605987549\n",
      "6.710949420928955\n",
      "6.755150318145752\n",
      "6.611039638519287\n",
      "6.689751625061035\n",
      "6.869660377502441\n",
      "6.702718734741211\n",
      "6.884037494659424\n",
      "6.643885612487793\n",
      "6.882127285003662\n",
      "6.940103530883789\n",
      "6.588273525238037\n",
      "6.770620822906494\n",
      "6.651396751403809\n",
      "6.520318984985352\n",
      "6.871309280395508\n",
      "6.654468536376953\n",
      "6.645321846008301\n",
      "6.749845027923584\n",
      "6.684603691101074\n",
      "6.594573974609375\n",
      "6.706405162811279\n",
      "6.6954450607299805\n",
      "6.674075126647949\n",
      "6.668527126312256\n",
      "6.8594794273376465\n",
      "6.599691390991211\n",
      "6.7153191566467285\n",
      "6.826026439666748\n",
      "6.803791046142578\n",
      "6.7096848487854\n",
      "6.711906433105469\n",
      "6.530125617980957\n",
      "6.694274425506592\n",
      "6.723336696624756\n",
      "6.794591903686523\n",
      "6.673261642456055\n",
      "6.53581428527832\n",
      "6.616812705993652\n",
      "6.649526596069336\n",
      "6.635985374450684\n",
      "6.747880935668945\n",
      "6.722192287445068\n",
      "6.724905967712402\n",
      "6.63240909576416\n",
      "7.016345024108887\n",
      "6.761261940002441\n",
      "6.606398582458496\n",
      "6.505029678344727\n",
      "6.557002544403076\n",
      "6.886124610900879\n",
      "6.684743881225586\n",
      "6.744430065155029\n",
      "6.7762370109558105\n",
      "6.756879806518555\n",
      "6.573612213134766\n",
      "6.481638431549072\n",
      "6.6852216720581055\n",
      "6.695708751678467\n",
      "6.8375020027160645\n",
      "6.79576301574707\n",
      "6.69227933883667\n",
      "6.589666366577148\n",
      "6.7707414627075195\n",
      "6.766382217407227\n",
      "6.622243881225586\n",
      "6.627602577209473\n",
      "6.607023239135742\n",
      "6.904576778411865\n",
      "6.5848236083984375\n",
      "6.8219404220581055\n",
      "6.442299842834473\n",
      "6.555935859680176\n",
      "6.700965881347656\n",
      "6.6862263679504395\n",
      "6.589323997497559\n",
      "6.655686378479004\n",
      "6.6640143394470215\n",
      "6.782658100128174\n",
      "6.6267290115356445\n",
      "6.670267105102539\n",
      "6.551542282104492\n",
      "6.645116329193115\n",
      "6.582159519195557\n",
      "6.699306488037109\n",
      "6.815304279327393\n",
      "6.521961212158203\n",
      "6.482229232788086\n",
      "6.593652248382568\n",
      "6.605705261230469\n",
      "6.671517848968506\n",
      "6.614493370056152\n",
      "6.661191463470459\n",
      "6.8003034591674805\n",
      "6.935713291168213\n",
      "6.7135162353515625\n",
      "6.7753400802612305\n",
      "6.688808917999268\n",
      "6.606024742126465\n",
      "6.482477188110352\n",
      "6.6005377769470215\n",
      "6.585318565368652\n",
      "6.824603080749512\n",
      "6.564558029174805\n",
      "6.584731578826904\n",
      "6.718522548675537\n",
      "6.591739177703857\n",
      "6.632623195648193\n",
      "6.681048393249512\n",
      "6.633189678192139\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.decode(m.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "# we want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros(B,T,C) # bow = bag of words (averaging)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can be really efficient using matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B,T,T) @ (B,T,C) -----> (B,T,C)\n",
    "\n",
    "torch.allclose(xbow,xbow2) # identicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3 : use Softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # tokens from the past cannot communicate with future\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow,xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4 : self-attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channel\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "wei = q @ k.transpose(-2, -1) # (B,T,16) @ (B, 16, T)  ------> (B, T, T)  \n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Notes: ***\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionnally encodes tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modelling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5 # scaling to controle the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0632)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9891)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9755)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1023,  1.3987,  1.0764, -0.1497, -0.2601,  1.0896, -0.7415,\n",
       "           0.9481],\n",
       "         [-0.5935,  0.1866,  0.8442, -2.6846, -0.5995, -0.1049, -0.2314,\n",
       "          -0.5133],\n",
       "         [-1.1968, -0.6008, -0.7900,  1.1396,  2.5007,  0.5416,  0.2368,\n",
       "          -0.2689],\n",
       "         [-0.2726, -0.0992,  0.4969,  0.5409,  0.5688,  0.5101,  0.9770,\n",
       "           1.1494],\n",
       "         [ 0.9239,  0.0869,  0.7659, -1.2412, -1.0235, -0.1439,  1.1736,\n",
       "          -0.8299],\n",
       "         [-0.2004,  0.1792,  1.2278, -1.0577, -0.0860,  0.4595, -0.2013,\n",
       "           0.8051],\n",
       "         [ 0.4227,  1.0302,  0.4409, -2.7691,  0.6206,  0.8169, -1.8316,\n",
       "          -1.0899],\n",
       "         [-0.0164,  1.5899,  2.0445, -2.2490, -0.3421,  1.0581, -1.4024,\n",
       "           0.3886]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
       "         0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
       "         1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
       "         0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
